# Skills Library Architecture: Experiment 002 Brief

**To:** Head of Developer Experience
**From:** Jacob Prall
**Date:** 2026-02-25
**Standard Skills Library:** [github.com/jacobprall/snowflake-standard-skills-library](https://github.com/jacobprall/snowflake-standard-skills-library)

---

## Bottom Line

We ran Snowflake agent tasks on Cortex Code with its bundled skills. On the hardest test — building an end-to-end security incident tracker — bundled skills scored 61%. We then stripped the bundled skills and installed a restructured "standard skills library" into Cortex Code. Same runtime, same model, same test. Score: 92%.

To confirm this wasn't specific to Cortex Code, we ran the same test on Cursor with the standard library. Score: 92% again. **The library architecture is what matters, not the runtime.**

---

## The Test

We gave the agent a single prompt: *"Build me a pipeline that identifies security-related tickets, flags PII, enriches them with severity scores, and gives us a live dashboard — only the security team should see it."* This task requires the agent to chain four Snowflake domains: Cortex AI functions for classification, materialized tables and dynamic tables for the pipeline, masking and row access policies for security, and a Streamlit app for the dashboard.

The environment was pre-seeded with traps — a dynamic table that already has AI functions in its definition (a known cost anti-pattern), a suspended pipeline, and a masking policy with a broken role check. The agent doesn't know these exist. We scored against an 18-item checklist covering correctness, architecture, security, and production awareness.

Three configurations, same prompt:

| Configuration | Score | Critical Trap |
|--------------|-------|---------------|
| Cortex Code + Bundled Skills | 11/18 | Put AI functions in dynamic tables (re-run on every refresh — massive cost) |
| Cortex Code + Standard Library | 16.5/18 | Avoided anti-pattern, built correct architecture |
| Cursor + Standard Library | 16.5/18 | Avoided anti-pattern, built correct architecture |

After seeing Cortex Code's results with bundled skills, we stripped the bundled skills and installed the standard library into Cortex Code — the score jumped from 11/18 to 16.5/18. We then ran the same test on Cursor with the standard library: same 16.5/18. The 5.5-point gap is entirely from skills architecture.

---

## Why the Standard Library Wins

Two structural properties that bundled skills lack:

**1. Cross-domain routing.** The standard library uses a DAG (meta-router → domain routers → playbooks → primitives). When a task spans AI analytics + data security + app deployment, the agent reads all three domain routers before writing SQL. The current Bundled skills are loaded one-at-a-time via keyword matching — the agent never sees warnings from adjacent domains.

**2. Anti-pattern warnings at the point of use.** The AI primitives contain: *"NEVER put AI functions inside a dynamic table definition."* The agent reads this during normal routing. Bundled skills have no equivalent — `cortex-ai-functions` and `dynamic-tables` are separate skills that don't reference each other.

These aren't content gaps that can be fixed by adding more documentation to bundled skills. They're architectural: keyword matching fundamentally can't surface cross-domain warnings because it loads one skill at a time.

---

## Findings That Apply to All Skills

These patterns appeared across all configurations and represent general product improvements:

| Finding | Impact | Fix |
|---------|--------|-----|
| **No database scoping at session start** | Agent explores entire account (1,260 schemas) before narrowing. Finds wrong tables, leaks object names from other databases. | Prompt for target database at session start, or add a guardrail: "always USE DATABASE before exploring." |
| **AI function return shapes undocumented** | 3+ error-fix cycles from wrong assumptions (AI_SENTIMENT isn't a float, AI_EXTRACT needs ARRAY, AI_CLASSIFY uses `:labels[0]` not `:label`). | Add return shape examples to `cortex-ai-functions`. |
| **SYSTEM$CLASSIFY never used for PII** | All configurations rely on manual column-name guessing. Non-obvious PII (CUSTOMER_NAME) missed every time. | Make SYSTEM$CLASSIFY a required step in `data-policy` audit workflows. |
| **Pre-existing objects are invisible** | No configuration discovered a pre-seeded dynamic table with a known anti-pattern. `SHOW TABLES` misses dynamic tables. | Add "audit target schemas" step to construction playbooks; include `SHOW DYNAMIC TABLES`. |

---

## Recommendation

**Adopt the standard library's DAG architecture for Cortex Code's skill system.** The specific proposal:

1. **Replace keyword matching with hierarchical routing.** A meta-router SKILL.md that loads domain routers based on task intent, not keyword overlap. This is how the standard library works — and it's why both Cursor and Cortex Code produce the same results when using it.

2. **Add cross-domain guardrails to existing bundled skills now.** One line in `cortex-ai-functions` and `dynamic-tables` — *"NEVER put AI functions inside a dynamic table definition"* — would close the 5.5-point gap immediately. This is the lowest-effort, highest-impact change.

3. **Add return shape documentation to `cortex-ai-functions`.** Extraction path examples for every function. Eliminates the trial-and-error debugging observed across tests.

4. **Add a database scoping step to the default session initialization.** Either in the CLI itself or as a guardrail in the agent's system prompt.

---

## Appendix: Experiment Design

- **Test** covering cost investigation, disambiguation, audit-before-act, AI pipelines, error recovery, cross-domain builds, cost controls, and secure migration
- **Phase 1** — Cortex Code + bundled skills: completed T1–T6, scored 49/71 (69%)
- **Phase 2** — Stripped bundled skills, installed standard library into Cortex Code: completed T6, scored 16.5/18 (92%)
- **Phase 3** — Ran the same test on Cursor with the standard library: completed T6, scored 16.5/18 (92%)
- Every test includes pre-seeded traps designed to test whether the agent investigates before acting
- Full results: `experiments/002_cursor-vs-cortex/experiment_log.md`
- Detailed analysis: `summaries/02-24-26-report.md`
