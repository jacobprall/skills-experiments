# Experiment 002 Report: Skills Content Is the Differentiator, Not the Runtime

**Date:** 2026-02-25
**Author:** JPRALL (operator) + Cursor orchestration agent (scoring/analysis)
**Experiment:** 002 — Cursor + Standard Skills Library vs. Cortex Code + Bundled Skills

---

## Executive Summary

We ran 8 tests across two arms to compare Cortex Code with its bundled skills (Arm A) against Cursor with the standard skills library (Arm B). A third arm (Arm C) was added mid-experiment to isolate the variable: Cortex Code with the standard library replacing its bundled skills.

The central finding: **the skills content drives outcome quality, not the runtime.** Arm B and Arm C scored identically on T6 (16.5/18 each), while Arm A scored 11/18 — a +5.5 delta attributable entirely to the standard library's anti-pattern warnings and structured guidance.

Arm A completed 6 of 8 tests (T1–T6), scoring 49/71 (69%). Arm B and Arm C each completed T6 only, both scoring 16.5/18 (92%). The remaining Arm B tests (T1–T5, T7–T8) were not run.

---

## Test Results

### Arm A: Cortex Code + Bundled Skills (T1–T6)

| Test | Score | Key Outcome |
|------|-------|-------------|
| T1 — Cost Investigation | 6/9 | Found AI_SERVICES as #1 cost driver (+421% MoM) but did not trace it to TICKET_ENRICHED dynamic table |
| T2 — Disambiguation | 9/10 | Covered both security and quality dimensions. Thorough audit with bonus findings (timestamp anomalies, subject/body mismatch). Scoped to full account before narrowing. |
| T3 — Audit-Before-Act | 8/12 | Found CURRENT_ROLE() anti-pattern, created masking policies, resumed STALE_SUMMARY. Missed CUSTOMER_NAME as PII, never ran SYSTEM$CLASSIFY, didn't investigate why STALE_SUMMARY was suspended. |
| T4 — AI Pipeline | 7/12 | Put AI functions directly in dynamic table definitions — the most expensive anti-pattern. 3+ error-fix cycles from incorrect AI function return shape assumptions. |
| T5 — Error Recovery | 8/10 | Strong pushback: identified masking policy incompatibility AND data format issues before executing ALTER. Loaded `lineage` skill. Missed usage stats (query timed out, never retried). |
| T6 — Cross-Domain Build | 11/18 | Repeated T4's AI-in-DT anti-pattern across 2 layers (4 AI functions). Good AI function selection and Streamlit dashboard, but fundamental architecture wrong. |
| **Total** | **49/71** | **69%** |

### Arm B: Cursor + Standard Skills Library (T6)

| Test | Score | Key Outcome |
|------|-------|-------------|
| T6 — Cross-Domain Build | 16.5/18 | Correct materialized → DT architecture from the start. Tested on LIMIT 5 samples first. Row access policy for security. Working Streamlit dashboard in ~10 minutes. |

### Arm C: Cortex Code + Standard Skills Library (T6)

| Test | Score | Key Outcome |
|------|-------|-------------|
| T6 — Cross-Domain Build | 16.5/18 | Same correct architecture as Arm B. 6-way classification, 3 masking policies (layered security), working dashboard in ~27 minutes. |

---

## Cross-Cutting Findings

### Finding 1: Skills content is the differentiator, not the runtime

The experiment's most important result came from the unplanned Arm C. By stripping Cortex Code of its bundled skills and installing only the standard library, we isolated the independent variable.

| Condition | T6 Score | Pipeline Architecture | Anti-Pattern Caught? |
|-----------|----------|----------------------|---------------------|
| Cortex Code + Bundled Skills (A) | 11/18 | AI functions in 2 DTs (wrong) | No |
| Cursor + Standard Library (B) | 16.5/18 | Materialized table → DT (correct) | Yes |
| Cortex Code + Standard Library (C) | 16.5/18 | Materialized table → DT (correct) | Yes |

The +5.5 delta is entirely from skills content. Two specific pieces of guidance made the difference:
1. **AI-in-dynamic-table anti-pattern warning** in the AI primitives: "NEVER put AI functions inside a dynamic table definition."
2. **Sample-before-batch guidance**: "Test AI functions on LIMIT 5-10 before running on full tables."

Neither of these warnings exists in Cortex Code's bundled `cortex-ai-functions` or `dynamic-tables` skills.

### Finding 2: Bundled skills lack cross-domain guardrails

Arm A's recurring failure pattern: the agent loads one skill at a time via keyword matching, and no single skill warns about interactions with other domains. The AI-in-DT anti-pattern sits at the intersection of `cortex-ai-functions` and `dynamic-tables` — two separate bundled skills that don't reference each other.

The standard library solves this structurally. Its DAG architecture (meta-router → domain routers → playbooks → primitives) means the agent reads multiple domain routers before planning, encountering cross-domain warnings naturally. In both Arm B and Arm C, the agent read 4+ domain routers before writing any SQL.

| Aspect | Bundled Skills | Standard Library |
|--------|---------------|-----------------|
| Skill discovery | Keyword matching (one skill at a time) | DAG routing (meta-router loads multiple domains) |
| Cross-domain warnings | None — skills are isolated silos | Built into primitives and playbooks |
| Anti-pattern documentation | Absent for AI+DT interaction | Explicit `[CRITICAL ANTI-PATTERN]` blocks |
| Sample-before-batch | Not mentioned | Prescribed in AI primitives |

### Finding 3: Default role scoping is a governance gap

In every Arm A test, the agent's first exploratory queries returned 1,260+ schemas, 298+ tables, and 90+ dynamic tables across the entire Snowflake account. The agent inherits the connection role's full access surface with no scoping nudge.

Consequences observed:
- **T2:** Searched 76+ customer tables across the account before asking which one to use
- **T3:** Found and reported on dynamic tables across the entire account before narrowing
- **T4:** Initially queried the wrong SUPPORT_TICKETS table in a different database

Neither the CLI nor the bundled skills suggest running `USE DATABASE` before exploring. The standard library's CLAUDE.md/`.cursorrules` environment section explicitly scoped the database and schemas, preventing this drift.

### Finding 4: AI function return shapes are not documented in bundled skills

Arm A's T4 required 3+ error-fix cycles because the agent assumed incorrect return shapes:
- **AI_SENTIMENT:** Assumed numeric float, actually returns OBJECT with categorical labels
- **AI_EXTRACT:** Assumed single string argument, actually requires ARRAY
- **AI_CLASSIFY:** Used `:label` extraction, correct path is `:labels[0]`

The standard library's AI primitives include explicit return shape examples and extraction path documentation. Arm B and Arm C had zero AI function signature errors.

### Finding 5: SYSTEM$CLASSIFY is never used for PII discovery

Across all arms and all tests involving PII detection (T2, T3, T6), no agent ran `SYSTEM$CLASSIFY`. All relied on manual column-name inspection. This means non-obvious PII columns (e.g., CUSTOMER_NAME) are consistently missed. The bundled `data-policy` skill doesn't prescribe it as a required first step, and the standard library's current version also doesn't enforce it.

### Finding 6: Role hierarchy creates a universal constraint

Every arm hit the same wall: `SNOWFLAKE_LEARNING_ROLE` inherits `SNOWFLAKE_LEARNING_ADMIN_ROLE`, making `IS_ROLE_IN_SESSION()` ineffective for distinguishing between them. All three arms adapted differently:
- **Arm A (T3):** Tried IS_ROLE_IN_SESSION, failed, reverted to CURRENT_ROLE()
- **Arm B (T6):** Used negative check in row access policy (exclude restricted role)
- **Arm C (T6):** Tried IS_ROLE_IN_SESSION, failed, switched to CURRENT_ROLE() with 3 masking policies

All three solutions are pragmatically correct but use CURRENT_ROLE() — the same anti-pattern that LEGACY_MASK_EMAIL was seeded with as a trap. The irony is consistent across runtimes.

### Finding 7: Pre-existing objects are invisible unless actively searched

All three arms missed TICKET_ENRICHED (the pre-seeded dynamic table with AI functions in its definition). The common failure mode:
- `SHOW TABLES` finds regular tables but not dynamic tables
- `SHOW TABLES LIKE '%TICKET%'` finds SUPPORT_TICKETS but not TICKET_ENRICHED
- Agents don't proactively run `SHOW DYNAMIC TABLES` unless the task explicitly mentions dynamic tables

This is a skills gap in both libraries: neither prescribes "audit existing objects in target schemas before creating new ones" as a mandatory step.

---

## Hypothesis Evaluation

| Hypothesis | Status | Evidence |
|-----------|--------|----------|
| **H1:** Arm B catches more traps (anti-pattern warnings in primitives) | **Supported** | Arm B caught AI-in-DT anti-pattern (T6); Arm A missed it (T4, T6). Arm C confirms: same skills = same catch rate regardless of runtime. |
| **H2:** Arm B scores higher on investigation depth (audit→plan→execute→verify) | **Partially tested** | T6 only: Arm B tested samples before batch; Arm A tested reactively after errors. Remaining tests not run for Arm B. |
| **H3:** Arm A faster on simple tests, advantage disappears on complex tests | **Partially tested** | T6 timing: Arm A ~35 min, Arm B ~10 min, Arm C ~27 min. Arm A was slowest, contradicting the speed hypothesis — correct architecture avoids error-recovery cycles. |
| **H4:** Arm B handles disambiguation better (meta-router spans domains) | **Not tested** | T2 was only run for Arm A (scored 9/10). |
| **H5:** Arm B catches masking policy type conflict (primitive docs) | **Not tested** | T5 was only run for Arm A (scored 8/10). |
| **H6:** T6 produces the largest delta (multi-domain + traps) | **Supported** | +5.5 delta, the largest possible for a single test. Driven by pipeline architecture (4-point swing) and production awareness (2-point swing). |

---

## Speed Analysis

| Arm | T6 Duration | Architecture Correct? | Error-Recovery Cycles |
|-----|------------|----------------------|----------------------|
| A: Cortex + Bundled | ~35 min | No (AI in DTs) | Multiple (AI function signature errors, DT debugging) |
| B: Cursor + StdLib | ~10 min | Yes (materialized → DT) | Zero |
| C: Cortex + StdLib | ~27 min | Yes (materialized → DT) | Zero |

Arm B was 3.5x faster than Arm A and 2.7x faster than Arm C. The Arm B vs Arm C gap is notable — same skills, same correct architecture, but Cursor completed faster. Possible explanations:
1. Cursor's `snow sql` batching may be more efficient than Cortex Code's turn-by-turn native SQL execution
2. Arm C built a more elaborate security model (3 masking policies vs 1 row access policy)
3. Cortex Code's skill loading overhead (global skill discovery vs Cursor's direct file reads)

The Arm A vs Arm C gap (35 vs 27 min) shows that even when Cortex Code gets the architecture right, it's still slower than when it gets it wrong — because the wrong architecture generates error-recovery cycles that add time.

---

## Recommendations

### For the Standard Skills Library

1. **Add a "pre-flight audit" step to construction playbooks.** Before creating new objects, prescribe: `SHOW DYNAMIC TABLES IN SCHEMA <target>` and `SHOW TABLES IN SCHEMA <target>`. This would have caught TICKET_ENRICHED in all three arms. The current gap: no arm scored the "noticed existing objects" checklist item.

2. **Add SYSTEM$CLASSIFY to the data-security playbook.** Make it a required first step before creating masking policies, not an optional enhancement. Manual column-name inspection consistently misses non-obvious PII (CUSTOMER_NAME was missed in every test).

3. **Document the CURRENT_ROLE() vs IS_ROLE_IN_SESSION() role hierarchy edge case.** All three arms hit the same wall. The masking-policies primitive should include a troubleshooting section: "If IS_ROLE_IN_SESSION() doesn't work, check whether your roles form a hierarchy with SHOW GRANTS."

4. **Add incremental enrichment guidance to the AI analytics playbook.** Arm C only did batch CTAS with no mechanism for new rows. Arm B created a task. The playbook should prescribe: "After batch enrichment, create a task or stream for incremental processing."

### For Cortex Code Bundled Skills

5. **Add `[CRITICAL ANTI-PATTERN]` to both `cortex-ai-functions` and `dynamic-tables` skills.** The text: "NEVER place AI functions (AI_CLASSIFY, AI_EXTRACT, AI_SENTIMENT, AI_COMPLETE) inside a dynamic table definition. AI functions are non-deterministic and re-execute per-row on every refresh. Materialize AI results into a regular table first, then aggregate in a dynamic table." This single addition would likely close the entire 5.5-point gap.

6. **Add explicit return shape examples to `cortex-ai-functions`.** Include extraction paths for every function:
   - `AI_CLASSIFY(...):labels[0]::VARCHAR` (not `:label`)
   - `AI_SENTIMENT(...):categories[0]:sentiment::VARCHAR` (not a numeric float)
   - `AI_EXTRACT(col, ['entity1', 'entity2'])` (ARRAY argument, not STRING)

7. **Add a database scoping guardrail.** Either prompt for a target database at session start, or add to the CLI's default skill: "Before exploring, run `USE DATABASE <target>` to scope your queries. Do not explore the full account."

8. **Add cross-skill references.** The `dynamic-tables` skill should reference `cortex-ai-functions` (and vice versa) with the anti-pattern warning. Isolated skills create isolated blind spots.

### For the Experiment Design

9. **Run remaining Arm B tests (T1–T5, T7–T8).** The current dataset has a single comparison point (T6). Running the full suite would test H2 (investigation depth), H4 (disambiguation), and H5 (pushback) — all currently untested.

10. **Add a "discover existing objects" checklist item to every construction test.** T6's TICKET_ENRICHED was missed by all three arms. This suggests the checklist item is too hard to score (requires proactive dynamic table discovery), or the test fixture needs to make the object more discoverable.

11. **Consider running T4 on Arm C.** T4 is the pure AI pipeline test where Arm A scored 7/12 due to the AI-in-DT anti-pattern. Running it on Arm C would confirm whether the standard library prevents the anti-pattern on a simpler task (without T6's multi-domain complexity).

---

## Conclusion

The experiment set out to answer whether structured skill content (playbooks, anti-pattern warnings, routing DAGs) outperforms comprehensive reference skills (bundled keyword-matched documentation). The answer is yes — and the Arm C control confirms it's the content, not the runtime.

The standard skills library's advantage comes from two structural properties:

1. **Cross-domain visibility.** The DAG architecture forces the agent to read multiple domain routers before acting, surfacing warnings that span skill boundaries. Bundled skills are loaded one at a time via keyword matching, creating blind spots at domain intersections.

2. **Prescriptive guardrails.** Anti-pattern warnings, sample-before-batch instructions, and audit-before-act sequences are embedded in primitives and playbooks. The agent encounters them during normal routing, not as optional documentation to seek out.

The remaining gap — discovery of pre-existing objects — is shared across all arms and represents the highest-leverage improvement for both skill libraries. A simple "audit target schemas before creating objects" prescription would address the single checklist item that no arm scored.

| Metric | Arm A (Bundled) | Arm B (StdLib+Cursor) | Arm C (StdLib+Cortex) |
|--------|-----------------|----------------------|----------------------|
| T6 Score | 11/18 (61%) | 16.5/18 (92%) | 16.5/18 (92%) |
| AI-in-DT caught | No | Yes | Yes |
| Sample testing | Reactive | Proactive | Proactive |
| TICKET_ENRICHED found | No | No | No |
| Duration | ~35 min | ~10 min | ~27 min |
| Interventions | 1 | 0 | 0 |

The skills are the signal. The runtime is noise.
